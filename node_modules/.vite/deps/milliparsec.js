import {
  __commonJS,
  __toESM
} from "./chunk-5WRI5ZAA.js";

// browser-external:node:buffer
var require_node_buffer = __commonJS({
  "browser-external:node:buffer"(exports, module) {
    module.exports = Object.create(new Proxy({}, {
      get(_, key) {
        if (key !== "__esModule" && key !== "__proto__" && key !== "constructor" && key !== "splice") {
          console.warn(`Module "node:buffer" has been externalized for browser compatibility. Cannot access "node:buffer.${key}" in client code. See https://vite.dev/guide/troubleshooting.html#module-externalized-for-browser-compatibility for more details.`);
        }
      }
    }));
  }
});

// ../../../../../../../node_modules/milliparsec/dist/index.js
var import_node_buffer = __toESM(require_node_buffer());
var hasBody = (method) => ["POST", "PUT", "PATCH", "DELETE"].includes(method);
var defaultPayloadLimit = 102400;
var defaultErrorFn = (payloadLimit) => new Error(`Payload too large. Limit: ${payloadLimit} bytes`);
var p = (fn, payloadLimit = defaultPayloadLimit, payloadLimitErrorFn = defaultErrorFn) => async (req, _res, next) => {
  try {
    const body = [];
    for await (const chunk of req) {
      const totalSize = body.reduce((total, buffer) => total + buffer.byteLength, 0);
      if (totalSize > payloadLimit)
        throw payloadLimitErrorFn(payloadLimit);
      body.push(chunk);
    }
    return fn(import_node_buffer.Buffer.concat(body));
  } catch (e) {
    next === null || next === void 0 ? void 0 : next(e);
  }
};
var custom = (fn) => async (req, _res, next) => {
  if (hasBody(req.method))
    req.body = await p(fn)(req, _res, next);
  next === null || next === void 0 ? void 0 : next();
};
var json = ({ payloadLimit, payloadLimitErrorFn } = {}) => async (req, res, next) => {
  if (hasBody(req.method)) {
    req.body = await p((x) => {
      const str = td.decode(x);
      return str ? JSON.parse(str) : {};
    }, payloadLimit, payloadLimitErrorFn)(req, res, next);
  }
  next === null || next === void 0 ? void 0 : next();
};
var raw = ({ payloadLimit, payloadLimitErrorFn } = {}) => async (req, _res, next) => {
  if (hasBody(req.method)) {
    req.body = await p((x) => x, payloadLimit, payloadLimitErrorFn)(req, _res, next);
  }
  next === null || next === void 0 ? void 0 : next();
};
var td = new TextDecoder();
var text = ({ payloadLimit, payloadLimitErrorFn } = {}) => async (req, _res, next) => {
  if (hasBody(req.method)) {
    req.body = await p((x) => td.decode(x), payloadLimit, payloadLimitErrorFn)(req, _res, next);
  }
  next === null || next === void 0 ? void 0 : next();
};
var urlencoded = ({ payloadLimit, payloadLimitErrorFn } = {}) => async (req, _res, next) => {
  if (hasBody(req.method)) {
    req.body = await p((x) => Object.fromEntries(new URLSearchParams(x.toString()).entries()), payloadLimit, payloadLimitErrorFn)(req, _res, next);
  }
  next === null || next === void 0 ? void 0 : next();
};
var getBoundary = (contentType) => {
  const match = /boundary=(.+);?/.exec(contentType);
  return match ? `--${match[1]}` : null;
};
var defaultFileSizeLimitErrorFn = (limit) => new Error(`File too large. Limit: ${limit} bytes`);
var defaultFileSizeLimit = 200 * 1024 * 1024;
var parseMultipart = (body, boundary, { fileCountLimit, fileSizeLimit = defaultFileSizeLimit, fileSizeLimitErrorFn = defaultFileSizeLimitErrorFn }) => {
  const parts = body.split(new RegExp(`${boundary}(--)?`)).filter((part) => !!part && /content-disposition/i.test(part));
  const parsedBody = {};
  if (fileCountLimit && parts.length > fileCountLimit)
    throw new Error(`Too many files. Limit: ${fileCountLimit}`);
  parts.forEach((part) => {
    const [headers, ...lines] = part.split("\r\n").filter((part2) => !!part2);
    const data = lines.join("\r\n").trim();
    if (data.length > fileSizeLimit)
      throw fileSizeLimitErrorFn(fileSizeLimit);
    const name = /name="(.+?)"/.exec(headers)[1];
    const filename = /filename="(.+?)"/.exec(headers);
    if (filename) {
      const contentTypeMatch = /Content-Type: (.+)/i.exec(data);
      const fileContent = data.slice(contentTypeMatch[0].length + 2);
      const file = new File([fileContent], filename[1], { type: contentTypeMatch[1] });
      parsedBody[name] = parsedBody[name] ? [...parsedBody[name], file] : [file];
      return;
    }
    parsedBody[name] = parsedBody[name] ? [...parsedBody[name], data] : [data];
    return;
  });
  return parsedBody;
};
var multipart = ({ payloadLimit = Number.POSITIVE_INFINITY, payloadLimitErrorFn, ...opts } = {}) => async (req, res, next) => {
  if (hasBody(req.method)) {
    req.body = await p((x) => {
      const boundary = getBoundary(req.headers["content-type"]);
      if (boundary)
        return parseMultipart(td.decode(x), boundary, opts);
      return {};
    }, payloadLimit, payloadLimitErrorFn)(req, res, next);
  }
  next === null || next === void 0 ? void 0 : next();
};
export {
  custom,
  hasBody,
  json,
  multipart,
  p,
  raw,
  text,
  urlencoded
};
//# sourceMappingURL=milliparsec.js.map
